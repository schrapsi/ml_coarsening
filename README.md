# ML Coarsening: Machine Learning-Guided Graph Partitioning

This project implements machine learning techniques to improve coarsening for multilevel graph partitioning.

## 🎯 Project Goal

The primary goal is to enhance balanced graph partitioning by using machine learning to guide the coarsening phase of multilevel heuristics. Traditional coarsening methods (e.g., label-propagation clustering) rely only on local connectivity and can destroy global structure, limiting final partition quality. This project addresses this limitation by training deep neural networks to predict edge-cut probabilities and guide vertex clustering decisions.

## 📖 Project Description

### Background & Motivation

Balanced graph partitioning—dividing a graph into k roughly equal-weight blocks while minimizing inter-block edge weights—is NP-complete and central to applications like:
- High-performance computing (HPC) load balancing
- Protein-interaction network analysis  
- VLSI design

Multilevel heuristics remain the state of the art: coarsen the graph, compute an initial partition on the small graph, then uncoarsen with refinement. However, standard coarsening approaches have limitations in preserving global graph structure.

### Core Innovation: ML-Guided Coarsening

#### 1. Edge-Cut Probability Model
- Trains a deep neural network regressor `f_m: E → [0,1]` to predict the likelihood an edge will be cut in a high-quality final partition
- Uses 208-dimensional feature vectors per edge combining:
  - Global graph statistics
  - Edge-specific features (e.g., Jaccard index)
  - Vertex-specific statistics (e.g., degree, distribution moments)

#### 2. Guided Vertex Clustering
- During coarsening, prevents clustering along edges with high cut-probability predictions
- Excludes clusters where average cut-probability exceeds dynamic thresholds (0.2→0.8)
- Newly formed edges inherit weighted averages of their original edges' probabilities

#### 3. Network Architecture & Training
- **Architecture**: 3–5 hidden layers (e.g., 208 → 1549 → 2045 → 702 → 1) with sigmoid output
- **Training**: MSE loss, Adam optimizer, optional batch-norm/dropout, weight decay, early stopping
- **Labels**: Generated by running Mt-KaHyPar multiple times to record cut frequencies
- **Optimization**: Tree-structured Parzen Estimator over 300 trials with pruning

### Project Structure

```
ml_coarsening/
├── src/                     # Source code
│   ├── data/               # Data modules for different tasks
│   ├── models/             # Model implementations
│   ├── utils/              # Utility functions
│   ├── train.py            # Training script
│   └── inference.py        # Inference script
├── configs/                # Hydra configuration files
│   ├── experiment/         # Experiment configurations
│   ├── model/              # Model configurations
│   ├── data/               # Data configurations
│   └── trainer/            # Trainer configurations
├── scripts/                # Execution scripts
└── logs/                   # Training logs and MLflow tracking
```

## 🚀 Installation

1. **Clone the repository:**
```bash
git clone <repository-url>
cd ml_coarsening
```

2. **Install dependencies using uv:**
```bash
uv pip install -e .
```

3. **Activate environment (if using virtual environment):**
```bash
source .venv/bin/activate
```

## 🔧 Usage

### Basic Training Commands

#### Multi-Class Classification (Default)
```bash
uv run -m src.train experiment=multi_classification
```

#### Binary Classification
```bash
uv run -m src.train experiment=bin_classification
```

#### Regression with BCE Loss
```bash
uv run -m src.train experiment=regression_bce
```

### Advanced Training Options

#### Custom Configuration Override
```bash
uv run -m src.train experiment=multi_classification \
  model.focal_gamma=4.0 \
  trainer=gpu \
  data.graphs_file=configs/data/graphs/custom.txt
```

#### CPU Training
```bash
uv run -m src.train experiment=multi_classification trainer=cpu
```

#### Debug Mode
```bash
uv run -m src.train experiment=multi_classification debug=limit
```

### Inference

#### Run Inference on Trained Model
```bash
uv run -m src.inference +experiment=multi_classification
```

#### Using Inference Script
```bash
./scripts/exec_scripts/inference.sh
```

### Batch Experiments

#### Run Multiple Experiments
```bash
./scripts/exec_scripts/run_experiments.sh
```

#### Run Experiments with Predictions
```bash
./scripts/exec_scripts/run_exp_wpred.sh
```

#### Feature Analysis
```bash
./scripts/exec_scripts/feature_analysis.sh
```

### Available Experiment Configurations

| Experiment | Description | Configuration File |
|------------|-------------|-------------------|
| `multi_classification` | Multi-class edge cut prediction | `configs/experiment/multi_classification.yaml` |
| `bin_classification` | Binary edge cut prediction | `configs/experiment/bin_classification.yaml` |
| `regression_bce` | Regression with BCE loss | `configs/experiment/regression_bce.yaml` |
| `local_multi` | Local multi-class testing | `configs/experiment/local_multi.yaml` |
| `mss_1_20` | Specific dataset configuration | `configs/experiment/mss_1_20.yaml` |

### Model Types

- **Binary Classification**: Predicts whether an edge will be cut (binary outcome)
- **Multi-Class Classification**: Predicts cut probability in discrete bins
- **Regression**: Directly predicts continuous cut probability values

## 📊 Experiment Tracking

The project uses **MLflow** for experiment tracking. To view results:

```bash
mlflow ui
```

Navigate to the provided URL to view:
- Training metrics and validation scores
- Model parameters and hyperparameters
- Training artifacts and model checkpoints

## 🎛️ Configuration System

The project uses **Hydra** for configuration management. Key configuration directories:

- `configs/experiment/`: Pre-defined experiment setups
- `configs/model/`: Model architecture definitions
- `configs/data/`: Dataset and feature configurations
- `configs/trainer/`: Training parameters (GPU/CPU, callbacks, etc.)


## 🛠️ Development

### Adding New Experiments
1. Create new configuration in `configs/experiment/`
2. Define model and data configurations if needed
3. Run with: `uv run -m src.train experiment=your_experiment`

### Custom Features
- Modify feature extraction in `src/utils/`
- Update feature lists in `configs/data/features/`
- Adjust model input size accordingly

### Training
To train a model, run the following command:

```bash
uv run -m src.train
```
